{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nsff591/white-wine-data-engineering-rf-lightgbm-0-82?scriptVersionId=97815070\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Physical characteristics to determine wine quality","metadata":{"id":"fFVe0zUCxvit"}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{"id":"tkdwyVuZyPma"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np # making use of arrays\nimport matplotlib.pyplot as plt # plotting the data\nimport pandas as pd # importing the data\n\nfrom sklearn.model_selection import train_test_split # splitting the data\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder # feature scaling\n\nfrom sklearn.ensemble import RandomForestClassifier # Random forest model\nimport lightgbm as lgb # LightLGBM model\nfrom sklearn.metrics import mean_squared_log_error # regression metric to analyse accuracy of the regression model\nfrom sklearn.metrics import r2_score # another regression metric to analyse our accuracy\nimport seaborn as sns # to analyse the data before modelling\nfrom sklearn.decomposition import KernelPCA # dimensional reduction technique\nimport pickle # this will allow us to save and load variables in a pickle file for later use\nfrom tensorflow.keras.models import Sequential # deep learning library\nfrom tensorflow.keras.layers import Dense, Dropout # deep learning library\nfrom tensorflow import keras # optimizer in neural network\nfrom sklearn.cluster import KMeans # kmeans clustering\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report # analysing the results\nfrom sklearn.model_selection import cross_val_score # k-fold cross validation\n\nprint(\"Importing Complete!\")","metadata":{"id":"FSmDR2FByO3A","outputId":"0c35babd-bc12-494d-e257-4d92c80f344b","execution":{"iopub.status.busy":"2022-04-14T20:51:53.158494Z","iopub.execute_input":"2022-04-14T20:51:53.159066Z","iopub.status.idle":"2022-04-14T20:51:53.168053Z","shell.execute_reply.started":"2022-04-14T20:51:53.159029Z","shell.execute_reply":"2022-04-14T20:51:53.167217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install hyperopt","metadata":{"id":"jD3FRG2k_Z6c","outputId":"29a6d74e-8b5d-4559-894e-b09c5151e0a6","execution":{"iopub.status.busy":"2022-04-14T20:51:53.203781Z","iopub.execute_input":"2022-04-14T20:51:53.2041Z","iopub.status.idle":"2022-04-14T20:52:02.334026Z","shell.execute_reply.started":"2022-04-14T20:51:53.204065Z","shell.execute_reply":"2022-04-14T20:52:02.333241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hyperopt import fmin, tpe, anneal, hp, space_eval, STATUS_OK #optimizing hyper parameters","metadata":{"id":"RmqCG_tj_KBt","execution":{"iopub.status.busy":"2022-04-14T20:52:02.336253Z","iopub.execute_input":"2022-04-14T20:52:02.336644Z","iopub.status.idle":"2022-04-14T20:52:02.70751Z","shell.execute_reply.started":"2022-04-14T20:52:02.336592Z","shell.execute_reply":"2022-04-14T20:52:02.706729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings \nwarnings.filterwarnings('ignore')","metadata":{"id":"SlX6IeCVvW5I","execution":{"iopub.status.busy":"2022-04-14T20:52:02.708614Z","iopub.execute_input":"2022-04-14T20:52:02.708857Z","iopub.status.idle":"2022-04-14T20:52:02.713033Z","shell.execute_reply.started":"2022-04-14T20:52:02.708829Z","shell.execute_reply":"2022-04-14T20:52:02.712148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing the Dataset","metadata":{"id":"n_bqAx_WybX1"}},{"cell_type":"code","source":"#importing the wine quality data\ndata = pd.read_csv('../input/white-wine-quality/winequality-white.csv', sep=';')\nprint(data.shape)\n\n# deleting NaN rows\ndata = data.dropna()\n#data = data.drop(['density','sulphates','pH'], axis=1)\nprint(str(data.shape) + \" -> New shape after dropping NaN rows\")\n\ndata.head(5)","metadata":{"id":"wKx0jsw1ycCN","outputId":"1ec218d3-92a8-4f3e-d6e5-cf86e57c658f","execution":{"iopub.status.busy":"2022-04-14T20:52:02.715058Z","iopub.execute_input":"2022-04-14T20:52:02.715304Z","iopub.status.idle":"2022-04-14T20:52:02.797982Z","shell.execute_reply.started":"2022-04-14T20:52:02.715279Z","shell.execute_reply":"2022-04-14T20:52:02.797204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysing the Data","metadata":{"id":"AEBKeA0iijdB"}},{"cell_type":"code","source":"feature_len = len(data.columns)-1\n\nn = 0\nrows = 3\ncolumns = 4\ni = 0\nj = 0\n\nfig, ax = plt.subplots(rows,columns,figsize= (10,6))\n\nwhile i < rows:\n  while j < columns:\n    if n == feature_len:\n      break \n    ax[i][j].hist(x = data.iloc[:,n].values,bins=20)\n    ax[i][j].set_title(data.columns[n])\n    n += 1\n    j += 1\n  i += 1\n  j = 0\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"sfFRzDHjB4Cz","outputId":"60fc39b4-1f39-47d4-d4e2-9eaa679cf52a","execution":{"iopub.status.busy":"2022-04-14T20:52:02.799228Z","iopub.execute_input":"2022-04-14T20:52:02.799433Z","iopub.status.idle":"2022-04-14T20:52:04.551826Z","shell.execute_reply.started":"2022-04-14T20:52:02.799409Z","shell.execute_reply":"2022-04-14T20:52:04.551067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analysing these histograms we can conclude several things:\n\n* Alcohol value has a high variance as the data is spread out and barely aligning with a gaussian distribution\n* Residual sugar is skewed right, I tried using a Log Transformation on it, but this did not improve the precision of models","metadata":{"id":"GLk2fd5EizPp"}},{"cell_type":"code","source":"data.describe()","metadata":{"id":"1ITG7Rg4nc2a","outputId":"50bcff42-83c5-4f8c-d1c5-52b2ae3ff7e4","execution":{"iopub.status.busy":"2022-04-14T20:52:04.552939Z","iopub.execute_input":"2022-04-14T20:52:04.553157Z","iopub.status.idle":"2022-04-14T20:52:04.602687Z","shell.execute_reply.started":"2022-04-14T20:52:04.553131Z","shell.execute_reply":"2022-04-14T20:52:04.601944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_len = len(data.columns)-1\n\nn = 0\nrows = 3\ncolumns = 4\ni = 0\nj = 0\n\nfig, ax = plt.subplots(rows,columns,figsize= (10,6))\n\nwhile i < rows:\n  while j < columns:\n    if n == feature_len:\n      break \n    sns.barplot(x = 'quality', y = data.columns[n], data= data, ci= 95, ax= ax[i][j])\n    ax[i][j].set_title(data.columns[n])\n    n += 1\n    j += 1\n  i += 1\n  j = 0\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"i5FsL6XFfykD","outputId":"69c6a347-65d1-4905-b13c-8d28cb54e826","execution":{"iopub.status.busy":"2022-04-14T20:52:04.603872Z","iopub.execute_input":"2022-04-14T20:52:04.604168Z","iopub.status.idle":"2022-04-14T20:52:09.258515Z","shell.execute_reply.started":"2022-04-14T20:52:04.604113Z","shell.execute_reply":"2022-04-14T20:52:09.257766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To further clarify the interpretation of these graphs: \n\n\n*   The black lines on the bars indicate that we can assure 95%(= confidence interval) of our data is within this number that our bar reaches.\n*   If we see no change in the height of the bar(=mean) regardless of the quality, we can interpret little correlation between those features.\n*   If we do see higher/lower height if the quality increases, we can assume higher correlation between these features.\n\nWe can conclude the following: \n\n\n1.   **Alcohol increases quality**\n2.   **Chlorides decreases quality**\n3.   **Density, Ph, Sulphates do not influence quality a lot**\n\n\n","metadata":{"id":"INSPeFHyxhyq"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(data.corr(), annot=True, robust=True)\nplt.show()","metadata":{"id":"Xtv-ktRCylc9","outputId":"a16255d3-49d5-40d1-8ade-32c656fc6f1c","execution":{"iopub.status.busy":"2022-04-14T20:52:09.259875Z","iopub.execute_input":"2022-04-14T20:52:09.260288Z","iopub.status.idle":"2022-04-14T20:52:10.260188Z","shell.execute_reply.started":"2022-04-14T20:52:09.260246Z","shell.execute_reply":"2022-04-14T20:52:10.259268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n1.   Density and Sugar 0.84 correlation (=high)\n2.   Density and alcohol -0.78 reversed correlation (=high negative)\n3.  **Alcohol has highest correlation of 0.44 with quality**\n","metadata":{"id":"fCaUFyity4SN"}},{"cell_type":"code","source":"def all_feature_scatter_plot():\n  feature_len = len(data.columns)-1\n\n  n = 0\n  rows = 3\n  columns = 4\n  i = 0\n  j = 0\n\n  fig, ax = plt.subplots(rows,columns,figsize= (10,6))\n\n  while i < rows:\n    while j < columns:\n      if n == feature_len:\n        break \n      ax[i][j].scatter(y = data.iloc[:,n].values,x = data['quality'])\n      ax[i][j].set_title(data.columns[n])\n      n += 1\n      j += 1\n    i += 1\n    j = 0\n\n  plt.tight_layout()\n  plt.show()\n\nall_feature_scatter_plot()","metadata":{"id":"uYgOKCZfhyxl","outputId":"19f8cb1c-21aa-418b-a355-643e48962561","execution":{"iopub.status.busy":"2022-04-14T20:52:10.261455Z","iopub.execute_input":"2022-04-14T20:52:10.261803Z","iopub.status.idle":"2022-04-14T20:52:11.791879Z","shell.execute_reply.started":"2022-04-14T20:52:10.261765Z","shell.execute_reply":"2022-04-14T20:52:11.790898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As alcohol has the highest correlation with quality, we will heavily look at their outliers here.\n\n*   **Alcohol has an outlier at the 9 quality mark**\n*   free sulfur dioxide has an outlier at 3 quality (this might impact correlation with alcohol of -0.25)\n*   Density, Fixed acidity, residual sugar and citric acid has some outliers at **6 quality**   \n\nDuring my first tries on some models, I noticed a lower precision at quality 6. These outliers could explain it.\n\n","metadata":{"id":"vTSBh5JsiVo9"}},{"cell_type":"markdown","source":"## Removing outliers","metadata":{"id":"jqJ7-6le1z8r"}},{"cell_type":"code","source":"import operator\n#{'>': operator.gt,\n#'<': operator.lt,\n#'>=': operator.ge,\n#'<=': operator.le,\n#'==': operator.eq}\n\ndef remove_outlier(label, quality_number, target_cutoff, eq):\n  n = 0\n  lst = data.loc[:,[label,'quality']]\n  lst_len = data.shape[0]\n\n  while n < lst_len:\n    if lst.iloc[n]['quality'] == quality_number and eq(lst.iloc[n][label],target_cutoff):\n      print(\"old value: \"+ str(data.loc[:,[label,'quality']].iloc[n][label]))\n      data.loc[n,label] = data.groupby('quality').mean()[label][quality_number]\n      print(\"new value: \"+ str(data.loc[:,[label,'quality']].iloc[n][label]))\n    n += 1","metadata":{"id":"IwRmfVgc41Ag","execution":{"iopub.status.busy":"2022-04-14T20:52:11.795476Z","iopub.execute_input":"2022-04-14T20:52:11.795783Z","iopub.status.idle":"2022-04-14T20:52:11.803583Z","shell.execute_reply.started":"2022-04-14T20:52:11.79575Z","shell.execute_reply":"2022-04-14T20:52:11.803029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Alcohol","metadata":{"id":"Z8sHiNSn2HbP"}},{"cell_type":"code","source":"remove_outlier('alcohol', 9, 11, operator.le)","metadata":{"id":"Qythg4UUdfCl","outputId":"9a09f202-7533-4e33-dbdb-2a820993fa81","execution":{"iopub.status.busy":"2022-04-14T20:52:11.804537Z","iopub.execute_input":"2022-04-14T20:52:11.805209Z","iopub.status.idle":"2022-04-14T20:52:12.356912Z","shell.execute_reply.started":"2022-04-14T20:52:11.80517Z","shell.execute_reply":"2022-04-14T20:52:12.355912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Density","metadata":{"id":"HyEwwbR62KpG"}},{"cell_type":"code","source":"remove_outlier('density', 6, 1.02, operator.ge)","metadata":{"id":"u9fIlTNv2MfR","outputId":"8cf905b8-791f-492e-fc8a-f28d6c490ef4","execution":{"iopub.status.busy":"2022-04-14T20:52:12.358376Z","iopub.execute_input":"2022-04-14T20:52:12.358679Z","iopub.status.idle":"2022-04-14T20:52:13.131307Z","shell.execute_reply.started":"2022-04-14T20:52:12.358648Z","shell.execute_reply":"2022-04-14T20:52:13.130499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Residual sugar","metadata":{"id":"aocGMhuP6mqU"}},{"cell_type":"code","source":"remove_outlier('residual sugar', 6, 40, operator.ge)","metadata":{"id":"8xOIqXh32ARx","outputId":"7ccfe03f-e004-4e9e-9d52-af7b9ad60557","execution":{"iopub.status.busy":"2022-04-14T20:52:13.13248Z","iopub.execute_input":"2022-04-14T20:52:13.132692Z","iopub.status.idle":"2022-04-14T20:52:13.901858Z","shell.execute_reply.started":"2022-04-14T20:52:13.132667Z","shell.execute_reply":"2022-04-14T20:52:13.900965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Citric acid","metadata":{"id":"EXHYGVAS7VUA"}},{"cell_type":"code","source":"remove_outlier('citric acid', 6, 1.4, operator.ge)","metadata":{"id":"yTnWYmJY7c9n","outputId":"23996d75-6d39-4501-c7b9-a8443fcf7134","execution":{"iopub.status.busy":"2022-04-14T20:52:13.902953Z","iopub.execute_input":"2022-04-14T20:52:13.903195Z","iopub.status.idle":"2022-04-14T20:52:14.670581Z","shell.execute_reply.started":"2022-04-14T20:52:13.903169Z","shell.execute_reply":"2022-04-14T20:52:14.669852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fixed acidity","metadata":{"id":"da_47RH_7yVk"}},{"cell_type":"code","source":"remove_outlier('fixed acidity', 6, 12, operator.ge)","metadata":{"id":"PsdR3VPK7z9w","outputId":"4ce076c8-2e93-48df-b8ed-6970cbdb97c8","execution":{"iopub.status.busy":"2022-04-14T20:52:14.671968Z","iopub.execute_input":"2022-04-14T20:52:14.672298Z","iopub.status.idle":"2022-04-14T20:52:15.430548Z","shell.execute_reply.started":"2022-04-14T20:52:14.67226Z","shell.execute_reply":"2022-04-14T20:52:15.429791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Total sulfur dioxide","metadata":{"id":"6o1aQWwT6-nE"}},{"cell_type":"code","source":"remove_outlier('total sulfur dioxide', 3, 260, operator.ge)","metadata":{"id":"_YW7HRJA7AfP","outputId":"32fbda59-de35-4b19-fcf5-aea628ee3905","execution":{"iopub.status.busy":"2022-04-14T20:52:15.432274Z","iopub.execute_input":"2022-04-14T20:52:15.43258Z","iopub.status.idle":"2022-04-14T20:52:15.966871Z","shell.execute_reply.started":"2022-04-14T20:52:15.432532Z","shell.execute_reply":"2022-04-14T20:52:15.966076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Free sulfur dioxide","metadata":{"id":"VVKlgArn8EVe"}},{"cell_type":"code","source":"remove_outlier('free sulfur dioxide', 3, 190, operator.ge)","metadata":{"id":"Ebe6t1fY8GqE","outputId":"780ff9b1-ae75-4af7-f1bc-9e831b456ffb","execution":{"iopub.status.busy":"2022-04-14T20:52:15.96809Z","iopub.execute_input":"2022-04-14T20:52:15.968317Z","iopub.status.idle":"2022-04-14T20:52:16.491152Z","shell.execute_reply.started":"2022-04-14T20:52:15.96829Z","shell.execute_reply":"2022-04-14T20:52:16.490407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_feature_scatter_plot()","metadata":{"id":"kDknIqA7_mi_","outputId":"03eca2c4-3b02-46a3-e566-ea78e77d8996","execution":{"iopub.status.busy":"2022-04-14T20:52:16.492365Z","iopub.execute_input":"2022-04-14T20:52:16.492578Z","iopub.status.idle":"2022-04-14T20:52:17.963141Z","shell.execute_reply.started":"2022-04-14T20:52:16.492553Z","shell.execute_reply":"2022-04-14T20:52:17.961957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After removing most of the outliers we check the new scatterplot to confirm the imporoved data.","metadata":{"id":"iRUKby_1386y"}},{"cell_type":"markdown","source":"## Binning","metadata":{"id":"aoccvUWeZtXd"}},{"cell_type":"markdown","source":"Binning is a technique to split data in a feature into groups and replacing the original data with those groups.\n\nWhy would would you do that? Don't you lose information with it?\n\nYes, you lose information but you can make the model more robust, which can amplify the correlations in your model. The same principle is used in neural networks, where they drop a certain amount of data after a few hidden layers, to make the model more robust and less prune to overfitting.","metadata":{"id":"oVkZyHMA0BVw"}},{"cell_type":"markdown","source":"### K-means algorithm to determine bins","metadata":{"id":"gR8Ie3oHZ3Xu"}},{"cell_type":"markdown","source":"I first thought about splitting the Alcohol data into 3 parts as it is easily distinguishable in the histogram. But after testing k-means clustering on it. I noticed 2 extra clusters which i wouldn't have before. (cluster 4 and 5 on the clustering graph below)\n\nThis led me to start coding the binning based on K-means clustering.\n\nIt turned out to be a good choice as it increased my accuracy by 10-15%. A HUGE INCREASE\n\nUpdate: I later noticed I used Quality as part of my K-means algorithm. I shouldn't have done this as we are predicting the quality and we won't know it during the inferencing of our model. It could still be used to show the clusters and based on that we could hardcode some binning. But just remember that k-means shouldn't use the value we are predicting in the inferencing.","metadata":{"id":"yiXVIxjg1aR2"}},{"cell_type":"code","source":"alcohol = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 753)\n    kmeans.fit(data.loc[:,['alcohol','quality']])\n    alcohol.append(kmeans.inertia_)\nplt.plot(range(1, 11), alcohol)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('alcohol')\nplt.show()","metadata":{"id":"esZGBlUzae7m","outputId":"e8aa3bb7-6622-467d-db6a-71b8a5aead05","execution":{"iopub.status.busy":"2022-04-14T20:52:17.964545Z","iopub.execute_input":"2022-04-14T20:52:17.964792Z","iopub.status.idle":"2022-04-14T20:52:29.682897Z","shell.execute_reply.started":"2022-04-14T20:52:17.964761Z","shell.execute_reply":"2022-04-14T20:52:29.682212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I chose for 5 clusters as I noticed it to be a goog inflection point in the graph. I also tried 3 clusters, as this was my first precognition of the alcohol data but it returned 3-5% less accuracy so I reverted that idea.","metadata":{"id":"bPzGcX8G2Fk0"}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 951)\ny_kmeans = kmeans.fit_predict(data.loc[:,['alcohol','quality']])","metadata":{"id":"1m4JquihbZG9","execution":{"iopub.status.busy":"2022-04-14T20:52:29.686618Z","iopub.execute_input":"2022-04-14T20:52:29.688773Z","iopub.status.idle":"2022-04-14T20:52:31.055784Z","shell.execute_reply.started":"2022-04-14T20:52:29.688735Z","shell.execute_reply":"2022-04-14T20:52:31.05487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_means_data= data.loc[:,['alcohol','quality']]\nplt.scatter(k_means_data.iloc[y_kmeans == 0, 0], k_means_data.iloc[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\nplt.scatter(k_means_data.iloc[y_kmeans == 1, 0], k_means_data.iloc[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(k_means_data.iloc[y_kmeans == 2, 0], k_means_data.iloc[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(k_means_data.iloc[y_kmeans == 3, 0], k_means_data.iloc[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(k_means_data.iloc[y_kmeans == 4, 0], k_means_data.iloc[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('Cluster of Alcohol%')\nplt.xlabel('Alcohol')\nplt.ylabel('Quality')\nplt.legend()\nplt.show()","metadata":{"id":"oAbfqp_QbtPC","outputId":"0e12277e-b785-42bc-cc53-9052b869975c","execution":{"iopub.status.busy":"2022-04-14T20:52:31.064249Z","iopub.execute_input":"2022-04-14T20:52:31.064723Z","iopub.status.idle":"2022-04-14T20:52:32.081507Z","shell.execute_reply.started":"2022-04-14T20:52:31.064682Z","shell.execute_reply":"2022-04-14T20:52:32.080825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Binning Alcohol","metadata":{"id":"6FrPqgD6d0lP"}},{"cell_type":"code","source":"def binning(label, n_clusters):\n  kmeans = KMeans(n_clusters = n_clusters, init = 'k-means++', random_state = 951)\n  y_kmeans = kmeans.fit_predict(data.loc[:,[label,'quality']])\n\n  n = 0\n  \n  while n < data.shape[0]:\n    data.loc[n,label] = y_kmeans[n]\n    n += 1\n\nbinning('alcohol',5)","metadata":{"id":"YVbXMgGQd0Dr","execution":{"iopub.status.busy":"2022-04-14T20:52:32.085036Z","iopub.execute_input":"2022-04-14T20:52:32.085482Z","iopub.status.idle":"2022-04-14T20:52:34.513553Z","shell.execute_reply.started":"2022-04-14T20:52:32.085448Z","shell.execute_reply":"2022-04-14T20:52:34.512774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(y = data['alcohol'],x = data['quality'])\nplt.ylabel(\"Alcohol\")\nplt.xlabel(\"Quality\")\nplt.title(\"Alcohol after binning\")\nplt.show()","metadata":{"id":"a2ey20oXh0Px","outputId":"64fb6b07-39a0-448a-f0cc-05f9c1a0c179","execution":{"iopub.status.busy":"2022-04-14T20:52:34.5148Z","iopub.execute_input":"2022-04-14T20:52:34.515107Z","iopub.status.idle":"2022-04-14T20:52:34.704816Z","shell.execute_reply.started":"2022-04-14T20:52:34.515071Z","shell.execute_reply":"2022-04-14T20:52:34.703211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice how low alcohol still produced a high quality cluster. I would have never guessed this purely on a histogram.","metadata":{"id":"w3qeNWkA29td"}},{"cell_type":"code","source":"plt.hist(data['alcohol'])\nplt.show()","metadata":{"id":"6tcaSF66icQK","outputId":"e85b443b-adb4-4129-f594-18a5cd0b7fc2","execution":{"iopub.status.busy":"2022-04-14T20:52:34.705837Z","iopub.execute_input":"2022-04-14T20:52:34.706034Z","iopub.status.idle":"2022-04-14T20:52:34.871423Z","shell.execute_reply.started":"2022-04-14T20:52:34.706009Z","shell.execute_reply":"2022-04-14T20:52:34.870496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Binning alcohol had a big impact on accuracy**\n\nAlcohol has the highest correlation with quality, which gave me the hint of binning it. This made the data more robust which increased Accuracy of the model in several models.","metadata":{"id":"UWVxw5-8jKyP"}},{"cell_type":"markdown","source":"## Log Transformation","metadata":{"id":"0S_WjSh-wkw1"}},{"cell_type":"markdown","source":"I wanted to use the log transformation on residual sugar and chloride but after some experiments, it did not improve the accuracy of the model. \n\nI did see 2 clusters of logged data appear in residual sugar and I could have binned this data, but I didn't think it was worth the time as I already showed that I know how to bin data.","metadata":{"id":"8Z70cooxwoxV"}},{"cell_type":"markdown","source":"# Splitting the data in training and test set + Shuffling","metadata":{"id":"hMvpR-qh5UEM"}},{"cell_type":"code","source":"# splitting the features(x) with the expected result(y) (quality of wine)\nx = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values","metadata":{"id":"kPGFepZWZpQ8","execution":{"iopub.status.busy":"2022-04-14T20:52:34.872684Z","iopub.execute_input":"2022-04-14T20:52:34.872887Z","iopub.status.idle":"2022-04-14T20:52:34.877141Z","shell.execute_reply.started":"2022-04-14T20:52:34.872863Z","shell.execute_reply":"2022-04-14T20:52:34.876541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=555)\n\nprint(\"Unscaled training data example:\" + np.array2string(x_train[0], formatter={'float_kind':lambda x: \"%.0f\" % x}))","metadata":{"id":"p2EsCRHK5UNm","outputId":"8e0de2ea-a51a-4866-af54-9b9126ba1e0e","execution":{"iopub.status.busy":"2022-04-14T20:52:34.878264Z","iopub.execute_input":"2022-04-14T20:52:34.878632Z","iopub.status.idle":"2022-04-14T20:52:34.891972Z","shell.execute_reply.started":"2022-04-14T20:52:34.878605Z","shell.execute_reply":"2022-04-14T20:52:34.891432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feauture Scaling","metadata":{"id":"VwuSbiCm6b94"}},{"cell_type":"markdown","source":"We feature scale as most models like K-means, calculate the distance between points and some features have vastly different ranges at which their numbers can appear. By scaling the features, it will diminish the difference and cause a better performance on the models.\n\nWe standardize instead of normalize because we want the outliers to have less of an effect. Standardisation lowers this effect. Normalisation amplifies it.","metadata":{"id":"E-431EANxs8d"}},{"cell_type":"code","source":"# saving unscaled versions for testing purposes if needed\nunscaled_x_train = x_train\nunscaled_x_test = x_test\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\nprint(\"Scaled training data example:\" + np.array2string(x_train[0], formatter={'float_kind':lambda x: \"%.3f\" % x}))","metadata":{"id":"s_hNBZSn6cGE","outputId":"d074fbd1-73de-4988-e2ff-feca7e8f5893","execution":{"iopub.status.busy":"2022-04-14T20:52:34.893114Z","iopub.execute_input":"2022-04-14T20:52:34.89333Z","iopub.status.idle":"2022-04-14T20:52:34.905859Z","shell.execute_reply.started":"2022-04-14T20:52:34.893306Z","shell.execute_reply":"2022-04-14T20:52:34.905108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models: LightGBM model Classification","metadata":{"id":"X8GXEXVAMLCO"}},{"cell_type":"markdown","source":"I wanted to try the lightGBM boosting model as it recently got a lot of popularity with it's fast performance and low memory usage. It is said that i will also perform better accuracy if you align your parameters correctly.\n\nThis is why I tried using it. \n\nIt's weakness however, it is prune to overfitting.\n\nI will use randomforrest model after this one because it is less prune to overfitting and has a low amount of parameters. On top of that, it uses bagging instead of boosting which means that I have 2 examples of ensemble methods.","metadata":{"id":"RxjgbDE35SW6"}},{"cell_type":"markdown","source":"## Optimizing LightGBM parameters using Hyperopt","metadata":{"id":"RXPZni9ZQNzb"}},{"cell_type":"markdown","source":"Because LightGBM has an enormous amount of parameters, I used hyperopt to find the best one. But for those who don't want to go through the hurdle of optimizing hyper parameters. The default parameters gave me 1-2% less accuracy which is not that big of a difference.","metadata":{"id":"7CFRrTmO6mJw"}},{"cell_type":"code","source":"space = {'objective': 'multiclass',\n         'metric': 'multi_logloss',\n         'boosting': 'gbdt',\n         'num_iterations': hp.choice('num_iterations', range(50, 2000, 5)),\n         'learning_rate': hp.uniform('learning_rate', 0.003, 0.2),\n         'num_leaves': hp.choice('num_leaves', range(5,150,1)),\n         'max_depth': hp.choice('max_depth', range(2,10,1)),\n         'min_data_in_leaf': hp.choice('min_data_in_leaf', range(10,400,5)),\n         'reg_alpha': hp.uniform('reg_alpha',0,1),\n         'reg_lambda': hp.uniform('reg_lambda',0,1),\n         'n_estimators': hp.choice('n_estimators', range(50,2000,10))}","metadata":{"id":"Rdevk1Pre0aa","execution":{"iopub.status.busy":"2022-04-14T20:52:34.912214Z","iopub.execute_input":"2022-04-14T20:52:34.912728Z","iopub.status.idle":"2022-04-14T20:52:34.924618Z","shell.execute_reply.started":"2022-04-14T20:52:34.912697Z","shell.execute_reply":"2022-04-14T20:52:34.924039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_array= []\n\ndef objective(params):\n\n  classifier_lgb = lgb.LGBMClassifier(**params)\n  classifier_lgb.fit(x_train, y_train)\n  y_pred=classifier_lgb.predict(x_test)\n  accuracy = accuracy_score(y_test, y_pred)\n\n  accuracy_array.append(accuracy)\n\n  return {'loss': -accuracy, 'status': STATUS_OK }\n\nbest = fmin(objective,\n    space=space,\n    algo=anneal.suggest,\n    max_evals=150)\n\n# Print best parameters for anneal algo\nbest_params = space_eval(space, best)\nprint(best_params)\n\nplt.plot(accuracy_array)\nplt.show()","metadata":{"id":"7BxNqtdUa3WM","outputId":"7247783d-97bc-4046-a45a-fdbe88629ee9","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-14T20:52:34.925478Z","iopub.execute_input":"2022-04-14T20:52:34.925941Z","iopub.status.idle":"2022-04-14T20:59:12.555634Z","shell.execute_reply.started":"2022-04-14T20:52:34.92591Z","shell.execute_reply":"2022-04-14T20:59:12.554694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving/loading best LightGBM parameters","metadata":{"id":"SeuV2yepocLF"}},{"cell_type":"markdown","source":"In case you don't want to calculate the parameters everytime. I recommend loading the pickle file.","metadata":{"id":"aPLloOy_7JJo"}},{"cell_type":"code","source":"pickle.dump(best_params, open( \"./best_params_lightgbm.p\", \"wb\" ))\npickle.dump(accuracy_array, open( \"./accuracy_plot_lightgbm.p\", \"wb\" ))","metadata":{"id":"46OwMvvL6bjd","execution":{"iopub.status.busy":"2022-04-14T20:59:12.557328Z","iopub.execute_input":"2022-04-14T20:59:12.557846Z","iopub.status.idle":"2022-04-14T20:59:12.563833Z","shell.execute_reply.started":"2022-04-14T20:59:12.557789Z","shell.execute_reply":"2022-04-14T20:59:12.563216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = pickle.load(open(\"./best_params_lightgbm.p\", \"rb\" ))\naccuracy_array = pickle.load(open(\"./accuracy_plot_lightgbm.p\", \"rb\" ))","metadata":{"id":"LfYu27mA820D","execution":{"iopub.status.busy":"2022-04-14T20:59:12.564894Z","iopub.execute_input":"2022-04-14T20:59:12.565246Z","iopub.status.idle":"2022-04-14T20:59:12.573044Z","shell.execute_reply.started":"2022-04-14T20:59:12.565198Z","shell.execute_reply":"2022-04-14T20:59:12.572406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(accuracy_array)\nplt.show()","metadata":{"id":"IWk9JHlqA3FD","outputId":"3ec750ae-2b93-4dca-e9df-74d323bc054c","execution":{"iopub.status.busy":"2022-04-14T20:59:12.574512Z","iopub.execute_input":"2022-04-14T20:59:12.575037Z","iopub.status.idle":"2022-04-14T20:59:12.732318Z","shell.execute_reply.started":"2022-04-14T20:59:12.574937Z","shell.execute_reply":"2022-04-14T20:59:12.73136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model making/fitting","metadata":{"id":"sDWDx7mpoowL"}},{"cell_type":"code","source":"classifier_lgb = lgb.LGBMClassifier(**best_params)","metadata":{"id":"Vtj7OkHlMqbg","execution":{"iopub.status.busy":"2022-04-14T20:59:12.733698Z","iopub.execute_input":"2022-04-14T20:59:12.73423Z","iopub.status.idle":"2022-04-14T20:59:12.738586Z","shell.execute_reply.started":"2022-04-14T20:59:12.734187Z","shell.execute_reply":"2022-04-14T20:59:12.737826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_lgb.fit(x_train, y_train)","metadata":{"id":"ESyJ9kTAMxIP","outputId":"2209f77e-148e-489f-b2d1-854863314b69","execution":{"iopub.status.busy":"2022-04-14T20:59:12.739807Z","iopub.execute_input":"2022-04-14T20:59:12.740258Z","iopub.status.idle":"2022-04-14T20:59:15.300544Z","shell.execute_reply.started":"2022-04-14T20:59:12.740217Z","shell.execute_reply":"2022-04-14T20:59:15.299981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=classifier_lgb.predict(x_test)","metadata":{"id":"Y9j6Mr4HM8H7","execution":{"iopub.status.busy":"2022-04-14T20:59:15.301437Z","iopub.execute_input":"2022-04-14T20:59:15.301963Z","iopub.status.idle":"2022-04-14T20:59:15.43811Z","shell.execute_reply.started":"2022-04-14T20:59:15.301932Z","shell.execute_reply":"2022-04-14T20:59:15.437421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(accuracy_score(y_test, y_pred))","metadata":{"id":"eX-ZKqSHNMuf","outputId":"43a3687c-8229-49b3-89f3-01d5f32436bf","execution":{"iopub.status.busy":"2022-04-14T20:59:15.439491Z","iopub.execute_input":"2022-04-14T20:59:15.440077Z","iopub.status.idle":"2022-04-14T20:59:15.452002Z","shell.execute_reply.started":"2022-04-14T20:59:15.440033Z","shell.execute_reply":"2022-04-14T20:59:15.451308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report=classification_report(y_test,y_pred)\nprint(report)","metadata":{"id":"pTMDd3owNwAS","outputId":"50183d22-d8ae-4e9a-ebf8-d4bab5425faa","execution":{"iopub.status.busy":"2022-04-14T20:59:15.453926Z","iopub.execute_input":"2022-04-14T20:59:15.454561Z","iopub.status.idle":"2022-04-14T20:59:15.467752Z","shell.execute_reply.started":"2022-04-14T20:59:15.454527Z","shell.execute_reply":"2022-04-14T20:59:15.466856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can conclude that the precision is lacking at a wine quality of 4 and 6. **(This issue was partially resolved by binning alcohol)**\n\n* 4 has a low sample size\n* If we find another algorithm which can cover the unprecision of 6, we can make an ensemble model to alleviate this issue.","metadata":{"id":"UeO8KHuCmBO5"}},{"cell_type":"markdown","source":"## Applying k-fold Cross validation","metadata":{"id":"0Nub6kXRcoMV"}},{"cell_type":"code","source":"accuracies = cross_val_score(estimator = classifier_lgb, X = x_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","metadata":{"id":"9UO_eTDOczix","outputId":"c7ce07a2-24fe-49d3-fc9c-55f202a2843d","execution":{"iopub.status.busy":"2022-04-14T20:59:15.468987Z","iopub.execute_input":"2022-04-14T20:59:15.469858Z","iopub.status.idle":"2022-04-14T20:59:42.003154Z","shell.execute_reply.started":"2022-04-14T20:59:15.469814Z","shell.execute_reply":"2022-04-14T20:59:42.002397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models: Random Forest Classifier","metadata":{"id":"7VG-Awsr0dnH"}},{"cell_type":"markdown","source":"I stated in LightGBM, I chose randomforest because it is a bagging ensemble method and it is less prune to overfitting. On top of that, it is simple to use and almost always gives decent accuracy.\n\nAs a sidenote: I also tried XGBoost and SVM classifier but these perfermoded 5-8% worse even after hyperparameter optimization.\n\nIt is logical that SVM performed worse as it is \"medium\" sized data with relatively small number of features. Which SVM is bad at.","metadata":{"id":"iCw7RcXE7kCt"}},{"cell_type":"markdown","source":"I did not optimize hyper parameters as it did not increase the accuracy by a significant amount","metadata":{"id":"wbAt4f0hpXuI"}},{"cell_type":"code","source":"classifier_rf = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\nclassifier_rf.fit(x_train, y_train)","metadata":{"id":"fx5JWRpo0dtR","outputId":"96bd1c56-f848-454f-979b-2fb2f1096d31","execution":{"iopub.status.busy":"2022-04-14T20:59:42.006603Z","iopub.execute_input":"2022-04-14T20:59:42.009902Z","iopub.status.idle":"2022-04-14T20:59:43.255616Z","shell.execute_reply.started":"2022-04-14T20:59:42.00986Z","shell.execute_reply":"2022-04-14T20:59:43.25492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier_rf.predict(x_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(accuracy_score(y_test, y_pred))","metadata":{"id":"ieqRKqar00F3","outputId":"c671199d-bd29-4858-d68c-db58223d4d88","execution":{"iopub.status.busy":"2022-04-14T20:59:43.256807Z","iopub.execute_input":"2022-04-14T20:59:43.257295Z","iopub.status.idle":"2022-04-14T20:59:43.301004Z","shell.execute_reply.started":"2022-04-14T20:59:43.257253Z","shell.execute_reply":"2022-04-14T20:59:43.300247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report=classification_report(y_test,y_pred)\nprint(report)","metadata":{"id":"gVRuPl7EnHbW","outputId":"484420cc-0720-497c-90b3-981a665cf8b6","execution":{"iopub.status.busy":"2022-04-14T20:59:43.302009Z","iopub.execute_input":"2022-04-14T20:59:43.302244Z","iopub.status.idle":"2022-04-14T20:59:43.315727Z","shell.execute_reply.started":"2022-04-14T20:59:43.302216Z","shell.execute_reply":"2022-04-14T20:59:43.314911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying k-fold Cross validation","metadata":{"id":"d7xc9Z1AphXA"}},{"cell_type":"code","source":"accuracies = cross_val_score(estimator = classifier_rf, X = x_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","metadata":{"id":"V7XpBfr1wrRV","outputId":"b4cf2c18-a9bc-44c7-adba-52127f0c83f3","execution":{"iopub.status.busy":"2022-04-14T20:59:43.316985Z","iopub.execute_input":"2022-04-14T20:59:43.317341Z","iopub.status.idle":"2022-04-14T20:59:54.719043Z","shell.execute_reply.started":"2022-04-14T20:59:43.317301Z","shell.execute_reply":"2022-04-14T20:59:54.718221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model: Neural network\n","metadata":{"id":"S_F4020OxL7_"}},{"cell_type":"markdown","source":"I was thinking about using neural networks as they can tackle almost any problem. The big downside is it's computing time and unpredictable outcome. If the netowrk doesn't recognize the problem or structure, it might get stuck on the wrong foot. \n\nThe network however works really well with missing data or outliers. But this is not the case for our data as we already removed most of those.","metadata":{"id":"ZjdukUAF_XR8"}},{"cell_type":"markdown","source":"## Re-initializing data to make sure encoding is correctly applied for the neural network","metadata":{"id":"IIwp1MIapoQH"}},{"cell_type":"markdown","source":"We first need to change the quality feature to an encoded one where quality 5 = [0,0,1,0,0,0,0] for example. This is necessary for the neural network as the output layer will need 7 outputs in a this classification problem.","metadata":{"id":"78SeL-Cs-jK4"}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\ny= ohe.fit_transform(y.reshape(-1,1)).toarray()","metadata":{"id":"ymOPDBY5_lXh","execution":{"iopub.status.busy":"2022-04-14T20:59:54.720313Z","iopub.execute_input":"2022-04-14T20:59:54.720539Z","iopub.status.idle":"2022-04-14T20:59:54.728258Z","shell.execute_reply.started":"2022-04-14T20:59:54.720512Z","shell.execute_reply":"2022-04-14T20:59:54.72755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=555)\n\nprint(\"Unscaled training data example:\" + np.array2string(x_train[0], formatter={'float_kind':lambda x: \"%.0f\" % x}))","metadata":{"id":"OydmvUW0BSYt","outputId":"36706090-ceaa-4cd8-ee2c-941f9680389b","execution":{"iopub.status.busy":"2022-04-14T20:59:54.729488Z","iopub.execute_input":"2022-04-14T20:59:54.729925Z","iopub.status.idle":"2022-04-14T20:59:54.7412Z","shell.execute_reply.started":"2022-04-14T20:59:54.729887Z","shell.execute_reply":"2022-04-14T20:59:54.740614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving unscaled versions for testing purposes if needed\nunscaled_x_train = x_train\nunscaled_x_test = x_test\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\nprint(\"Scaled training data example:\" + np.array2string(x_train[0], formatter={'float_kind':lambda x: \"%.3f\" % x}))","metadata":{"id":"mG-HLjNXB0TO","outputId":"fd15d763-a960-4be5-fd78-564737480bab","execution":{"iopub.status.busy":"2022-04-14T20:59:54.742317Z","iopub.execute_input":"2022-04-14T20:59:54.742708Z","iopub.status.idle":"2022-04-14T20:59:54.750744Z","shell.execute_reply.started":"2022-04-14T20:59:54.742667Z","shell.execute_reply":"2022-04-14T20:59:54.749837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initializing the hidden layers","metadata":{"id":"F4O4ztlQp18e"}},{"cell_type":"markdown","source":"On why I chose certain parameters: I actuatly don't just tried a bit with trial and error. batch size is usually 32 and I noticed that i didn't need more than 50 epochs to converge on my model. So I made it 75 just to be sure.\n\nThe amount of hidden layers I added didn't change much to the accuracy, compared to having 2 before. I left them as it is, as it worked out somehow. (Yes I googled on how to improve it and I tried some, but it really didn't change a lot)","metadata":{"id":"s8lWV3Nc_f0r"}},{"cell_type":"code","source":"batch_size = 32\nval_split = 0.1\nseed = 753\n\nnumber_of_filters = 32\nnumber_of_epochs = 75\nnumber_of_units = 50","metadata":{"id":"BvTYBZtAxUsa","execution":{"iopub.status.busy":"2022-04-14T20:59:54.752073Z","iopub.execute_input":"2022-04-14T20:59:54.752473Z","iopub.status.idle":"2022-04-14T20:59:54.758518Z","shell.execute_reply.started":"2022-04-14T20:59:54.752432Z","shell.execute_reply":"2022-04-14T20:59:54.757749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn = Sequential()\n\nnn.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\nnn.add(Dense(units=128,activation='relu'))\nnn.add(Dense(units=64,activation='relu'))\nnn.add(Dropout(0.25))\nnn.add(Dense(units=64,activation='relu'))\nnn.add(Dense(units=64,activation='relu'))\nnn.add(Dropout(0.5))\nnn.add(Dense(units=32,activation='relu'))\nnn.add(Dense(units=7,activation='softmax'))","metadata":{"id":"Ih4TiKgrxi5C","execution":{"iopub.status.busy":"2022-04-14T20:59:54.759903Z","iopub.execute_input":"2022-04-14T20:59:54.760371Z","iopub.status.idle":"2022-04-14T20:59:54.865602Z","shell.execute_reply.started":"2022-04-14T20:59:54.760333Z","shell.execute_reply":"2022-04-14T20:59:54.864899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I changed the learning rate to 0.0002 because otherwise the model would overfit way too quickly with the default adam value of 0.001.","metadata":{"id":"E5lvN1UwA3mP"}},{"cell_type":"code","source":"opt = keras.optimizers.Adam(learning_rate=0.0002)\nnn.compile(optimizer= opt, loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{"id":"qohU-WUxyGVb","execution":{"iopub.status.busy":"2022-04-14T20:59:54.866678Z","iopub.execute_input":"2022-04-14T20:59:54.866892Z","iopub.status.idle":"2022-04-14T20:59:54.882964Z","shell.execute_reply.started":"2022-04-14T20:59:54.866868Z","shell.execute_reply":"2022-04-14T20:59:54.882365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fitting the model","metadata":{"id":"1qfpCRH5p-Wy"}},{"cell_type":"code","source":"fitted_model= nn.fit(x_train,y_train, validation_split= val_split, epochs= number_of_epochs, batch_size= batch_size)","metadata":{"id":"_imbm0rbyKxh","outputId":"4909da31-c776-4fbd-bc05-7bc9a1dd8f4f","execution":{"iopub.status.busy":"2022-04-14T21:00:00.644465Z","iopub.status.idle":"2022-04-14T21:00:00.644839Z","shell.execute_reply.started":"2022-04-14T21:00:00.644624Z","shell.execute_reply":"2022-04-14T21:00:00.644662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = nn.predict(x_test)\n\npred = list()\nfor i in range(len(y_pred)):\n  pred.append(np.argmax(y_pred[i]))\n\ntest = list()\nfor i in range(len(y_test)):\n  test.append(np.argmax(y_test[i]))","metadata":{"id":"jlG7SvX3CXnp","execution":{"iopub.status.busy":"2022-04-14T21:00:00.647045Z","iopub.status.idle":"2022-04-14T21:00:00.64755Z","shell.execute_reply.started":"2022-04-14T21:00:00.647281Z","shell.execute_reply":"2022-04-14T21:00:00.647305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = accuracy_score(pred,test)\nprint('Accuracy is:', a*100)","metadata":{"id":"Fm0IwGq2EBf-","outputId":"89c8f55a-93ee-4911-84d8-d7beac70c80a","execution":{"iopub.status.busy":"2022-04-14T21:00:00.648884Z","iopub.status.idle":"2022-04-14T21:00:00.649408Z","shell.execute_reply.started":"2022-04-14T21:00:00.649113Z","shell.execute_reply":"2022-04-14T21:00:00.64916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(test, pred)\nprint(cm)\nprint(accuracy_score(test, pred))","metadata":{"id":"ekjwkIG2F080","outputId":"9e6a4c89-503c-45bc-b633-b9a868a2fbf6","execution":{"iopub.status.busy":"2022-04-14T21:00:00.650646Z","iopub.status.idle":"2022-04-14T21:00:00.651095Z","shell.execute_reply.started":"2022-04-14T21:00:00.650854Z","shell.execute_reply":"2022-04-14T21:00:00.650876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report=classification_report(test,pred)\nprint(report)","metadata":{"id":"Zlo7lpiWMgFw","outputId":"b52cb03e-1709-4200-c050-2a5aa1ac893e","execution":{"iopub.status.busy":"2022-04-14T21:00:00.652689Z","iopub.status.idle":"2022-04-14T21:00:00.653182Z","shell.execute_reply.started":"2022-04-14T21:00:00.652901Z","shell.execute_reply":"2022-04-14T21:00:00.652924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysing the model on overfitting","metadata":{"id":"D3VH9DE7qER8"}},{"cell_type":"code","source":"def plot_metrics(history):\n  metrics = ['loss', 'accuracy']\n  for n, metric in enumerate(metrics):\n    try:\n      name = metric.replace(\"_\",\" \").capitalize()\n      plt.plot(history.epoch, history.history[metric], label='Train')\n      plt.plot(history.epoch, history.history['val_'+metric], linestyle=\"--\", label='Val')\n      plt.xlabel('Epoch')\n      plt.ylabel(name)\n      if metric == 'loss':\n        plt.ylim([0, plt.ylim()[1]])\n      elif metric == 'auc':\n        plt.ylim([0.8,1])\n      else:\n        plt.ylim([0,1])\n      plt.legend()\n      plt.show()  \n    except:\n      pass\nplot_metrics(fitted_model)\n\nplt.title(label='Zoomed in Accuracy plot')\nplt.plot(fitted_model.history[\"accuracy\"],label='Train')\nplt.plot(fitted_model.history[\"val_accuracy\"],linestyle=\"--\",label='Validation')\nplt.legend()\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","metadata":{"id":"T_WAFJNHEd1C","outputId":"77534c56-ad73-4b76-fcbb-7394a58b30ba","execution":{"iopub.status.busy":"2022-04-14T21:00:00.654567Z","iopub.status.idle":"2022-04-14T21:00:00.655025Z","shell.execute_reply.started":"2022-04-14T21:00:00.654779Z","shell.execute_reply":"2022-04-14T21:00:00.654802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our training accuracy is very close to our validation accuracy. This is a good indicator that we did not overfit our model.","metadata":{"id":"GHfS0iRvmXh1"}},{"cell_type":"markdown","source":"# Conclusion\n","metadata":{"id":"zighcnHXxvjG"}},{"cell_type":"markdown","source":"## Model Comparison","metadata":{"id":"vrYM9O3QH4Xv"}},{"cell_type":"markdown","source":"Model performance compared with **K-fold Cross Validation**:\n\n1.   Random forest -> Accuracy: **81.98 %** Standard Deviation: 1.50 %\n2.   lightGBM -> Accuracy: **81.42 %** Standard Deviation: 1.35 %\n3.   Neural network -> Accuracy is: **77.55 %**\n\nAlthough Random forest won with accuracy, lightGBM has a lower standard deviation. So I would compare them to be roughly the same accuracy as in some applications a lower Standard deviation is desired.\n\nThe neural network is the big loser here. Why?\n\nIf i let the neural network go for 200-500 epochs, I could get a higher accuracy but this would overfit the model drasticly.\n\nHow could I still improve the neural network without overfitting? My suggestion would be to increase the dataset. Neural networks work well with lots of data. 5000 rows where most of the features are very uncorrolated to the quality is just too little for the network to work efficiently on. \n\nI tried deleting uncorrolated features but this only lowered the accuracy about 1-2%, which is the undesired effect.\n\nI think I could still increase the accuracy of the Randomforest and LightGBM models if I data engineered more. An idea of mine was to log transform the residual sugar and then bin it into multiple groups. This might also work for density as it has a high corelation with alcohol.","metadata":{"id":"kx8xj_PCFdkG"}},{"cell_type":"markdown","source":"## Comparing the parameters","metadata":{"id":"or6ZjRjsIXUx"}},{"cell_type":"markdown","source":"I tried a lot of parameters manually on LightGBM but most of them resulted in the same outcome. Only after using hyperopt to optimise the parameters did I increase it's accuracy by 2%.\n\nRandomForest didn't need much parameter tuning, as the default settings almost always were the best choice.\n\nI do think that I could tune the neural network parameters more as I was pretty lousy on that front and didn't try using hyperopt.","metadata":{"id":"aYipvNiJIZZn"}},{"cell_type":"markdown","source":"## Noteworthy experiences","metadata":{"id":"LyzhygIKI5zx"}},{"cell_type":"markdown","source":"##### When I first put the raw data into the models, I received accuracies between 55% and 65%, which was horrible. After optimizing the parameters, it increased to 69-72%. And once I got to data engineering, mainly the binning of alcohol, it increased to 80-82%.\n\nAs many start their AI journey focussing on model improvement, they often forget the importance of data engineering. This is why I would recommend people to more analyse the Data and see if you can change it to your adventage.","metadata":{"id":"Q0rvW_ThJC63"}}]}